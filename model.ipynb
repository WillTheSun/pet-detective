{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cb37b355",
      "metadata": {},
      "source": [
        "# Pet Detective — Model Training\n",
        "\n",
        "Fine-tunes **MobileNetV2** (pretrained on ImageNet) on the [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) to classify 37 cat and dog breeds."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9911d70a",
      "metadata": {},
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "677413d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
        "from torchinfo import summary\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07190040",
      "metadata": {},
      "source": [
        "## 2. Dataset Download\n",
        "\n",
        "The Oxford-IIIT Pet Dataset contains 37 breeds (~200 images each).  \n",
        "`download=True` fetches and extracts it automatically on first run (~800 MB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3e0d9586",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: 37 | Train+val: 3680 | Test: 3669\n",
            "trainval: cached 3680 images → data/tensor_cache/trainval_tensors.pt\n",
            "test: cached 3669 images → data/tensor_cache/test_tensors.pt\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR   = './data'\n",
        "CACHE_DIR  = Path(DATA_DIR) / 'tensor_cache'\n",
        "IMG_SIZE   = 224  # MobileNetV2 input size\n",
        "CACHE_SIZE = 256  # cache at higher res so RandomResizedCrop has real spatial variety\n",
        "BATCH_SIZE = 32\n",
        "VAL_SPLIT  = 0.2\n",
        "\n",
        "# Download raw data (runs once)\n",
        "_raw      = OxfordIIITPet(root=DATA_DIR, split='trainval', target_types='category', download=True)\n",
        "_raw_test = OxfordIIITPet(root=DATA_DIR, split='test',     target_types='category', download=True)\n",
        "NUM_CLASSES = len(_raw.classes)\n",
        "print(f'Classes: {NUM_CLASSES} | Train+val: {len(_raw)} | Test: {len(_raw_test)}')\n",
        "\n",
        "# Cache stores values in [0, 1] — no normalization.\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((CACHE_SIZE, CACHE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def build_cache(split):\n",
        "    tensors_path = CACHE_DIR / f'{split}_tensors.pt'\n",
        "    labels_path  = CACHE_DIR / f'{split}_labels.pt'\n",
        "    if tensors_path.exists() and labels_path.exists():\n",
        "        print(f'{split}: cache already exists, skipping.')\n",
        "        return\n",
        "    CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    ds = OxfordIIITPet(root=DATA_DIR, split=split, target_types='category',\n",
        "                       download=False, transform=preprocess)\n",
        "    loader = DataLoader(ds, batch_size=64, shuffle=False, num_workers=4)\n",
        "    imgs_list, labels_list = [], []\n",
        "    for imgs, labels in loader:\n",
        "        imgs_list.append(imgs)\n",
        "        labels_list.append(labels)\n",
        "    torch.save(torch.cat(imgs_list),   tensors_path)\n",
        "    torch.save(torch.cat(labels_list), labels_path)\n",
        "    print(f'{split}: cached {len(ds)} images → {tensors_path}')\n",
        "\n",
        "build_cache('trainval')\n",
        "build_cache('test')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3778d48a",
      "metadata": {},
      "source": [
        "## 3. DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "db8d124e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 2944 | Val: 736 | Test: 3669\n",
            "Train batches: 92 | Val batches: 23 | Test batches: 115\n"
          ]
        }
      ],
      "source": [
        "class CachedPetDataset(Dataset):\n",
        "    \"\"\"Serves [0, 1] tensors from RAM; applies runtime transforms (augments + normalize).\"\"\"\n",
        "    def __init__(self, tensors, labels, augments=None):\n",
        "        self.tensors  = tensors\n",
        "        self.labels   = labels\n",
        "        self.augments = augments\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.tensors[idx]\n",
        "        if self.augments:\n",
        "            x = self.augments(x)\n",
        "        return x, self.labels[idx]\n",
        "\n",
        "_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                   std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# ColorJitter operates on [0, 1] values (as intended), then Normalize follows.\n",
        "# RandomResizedCrop pulls from the 256×256 cache → 224px output; scale=(0.75, 1.0) gives\n",
        "# crops ranging from ~192px to 256px, providing meaningful spatial diversity.\n",
        "train_augments = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.75, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.05),\n",
        "    transforms.RandomRotation(15),\n",
        "    _normalize,\n",
        "])\n",
        "\n",
        "# Val/test: center-crop to model input size, then normalize — no stochastic augments\n",
        "val_augments = transforms.Compose([\n",
        "    transforms.CenterCrop(IMG_SIZE),\n",
        "    _normalize,\n",
        "])\n",
        "\n",
        "# Load pre-processed tensors into RAM once (replaces per-epoch JPEG decode + resize + normalize)\n",
        "trainval_tensors = torch.load(CACHE_DIR / 'trainval_tensors.pt', weights_only=True)\n",
        "trainval_labels  = torch.load(CACHE_DIR / 'trainval_labels.pt',  weights_only=True)\n",
        "test_tensors     = torch.load(CACHE_DIR / 'test_tensors.pt',     weights_only=True)\n",
        "test_labels      = torch.load(CACHE_DIR / 'test_labels.pt',      weights_only=True)\n",
        "\n",
        "# Split train/val by index so each subset gets its own Dataset (and transform) cleanly\n",
        "n        = len(trainval_labels)\n",
        "perm     = torch.randperm(n)\n",
        "val_idx  = perm[:int(n * VAL_SPLIT)]\n",
        "train_idx = perm[int(n * VAL_SPLIT):]\n",
        "\n",
        "train_dataset = CachedPetDataset(trainval_tensors[train_idx], trainval_labels[train_idx], augments=train_augments)\n",
        "val_dataset   = CachedPetDataset(trainval_tensors[val_idx],   trainval_labels[val_idx],   augments=val_augments)\n",
        "test_dataset  = CachedPetDataset(test_tensors, test_labels,                                augments=val_augments)\n",
        "\n",
        "# num_workers=0: data is already in RAM, spawning workers only adds IPC overhead\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f'Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}')\n",
        "print(f'Train batches: {len(train_loader)} | Val batches: {len(val_loader)} | Test batches: {len(test_loader)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea64ef05",
      "metadata": {},
      "source": [
        "## 4. Model — MobileNetV2\n",
        "\n",
        "We load the ImageNet-pretrained backbone, **freeze all layers**, then replace the final classifier with a new head sized to our 37 classes.  \n",
        "Only the new head will be trained in the first phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "04190c30",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Freeze backbone\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace classifier head (in_features=1280 for MobileNetV2)\n",
        "in_features = model.classifier[1].in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.2),\n",
        "    # nn.Dropout(p=0.4),  # swap in above if train/val gap suggests more regularization\n",
        "    nn.Linear(in_features, NUM_CLASSES),\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "summary(model, input_size=(1, 3, IMG_SIZE, IMG_SIZE))\n",
        "model = model.to(device)  # summary() moves model to CPU internally; move it back"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2948de6b",
      "metadata": {},
      "source": [
        "## 5. Phase 1 — Train Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "6b8a8da7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | Train loss: 2.2578  acc: 0.530 | Val loss: 1.3669  acc: 0.823\n",
            "Epoch 02 | Train loss: 1.4032  acc: 0.804 | Val loss: 1.2271  acc: 0.852\n",
            "Epoch 03 | Train loss: 1.2996  acc: 0.833 | Val loss: 1.1798  acc: 0.860\n",
            "Epoch 04 | Train loss: 1.2381  acc: 0.847 | Val loss: 1.1716  acc: 0.865\n",
            "Epoch 05 | Train loss: 1.2015  acc: 0.857 | Val loss: 1.1721  acc: 0.856\n",
            "Epoch 06 | Train loss: 1.1483  acc: 0.887 | Val loss: 1.1337  acc: 0.882\n",
            "Epoch 07 | Train loss: 1.1375  acc: 0.896 | Val loss: 1.1277  acc: 0.885\n",
            "Epoch 08 | Train loss: 1.1501  acc: 0.889 | Val loss: 1.1336  acc: 0.885\n",
            "[Phase 1] Test loss: 1.1998  |  Test accuracy: 0.863\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 8\n",
        "LR = 1e-3\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=LR, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train() if train else model.eval()\n",
        "    total_loss, correct = 0.0, 0\n",
        "    ctx = torch.enable_grad() if train else torch.no_grad()\n",
        "    with ctx:\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            preds = model(images)\n",
        "            loss = criterion(preds, labels)\n",
        "            if train:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            total_loss += loss.item() * len(images)\n",
        "            correct += (preds.argmax(1) == labels).sum().item()\n",
        "    n = len(loader.dataset)\n",
        "    return total_loss / n, correct / n\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
        "    val_loss,   val_acc   = run_epoch(val_loader,   train=False)\n",
        "    scheduler.step()\n",
        "    print(f'Epoch {epoch:02d} | '\n",
        "          f'Train loss: {train_loss:.4f}  acc: {train_acc:.3f} | '\n",
        "          f'Val loss: {val_loss:.4f}  acc: {val_acc:.3f}')\n",
        "\n",
        "test_loss, test_acc = run_epoch(test_loader, train=False)\n",
        "print(f'[Phase 1] Test loss: {test_loss:.4f}  |  Test accuracy: {test_acc:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b00714a7",
      "metadata": {},
      "source": [
        "## 6. Phase 2 — Full Backbone Fine-tuning\n",
        "\n",
        "Head has converged; now unfreeze the entire backbone with very low LRs. Early layers (edges, textures) are already well-suited to natural images and barely need touching — the cosine schedule anneals both param groups smoothly to near-zero.\n",
        "\n",
        "- **Backbone**: 1e-6 — minimal nudge, preserves low-level pretrained features\n",
        "- **Head**: 1e-5 — still updating faster than backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "6d7c64ec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P2 Epoch 01 | Train loss: 1.1395  acc: 0.893 | Val loss: 1.1260  acc: 0.886\n",
            "P2 Epoch 02 | Train loss: 1.1132  acc: 0.905 | Val loss: 1.1192  acc: 0.883\n",
            "P2 Epoch 03 | Train loss: 1.1218  acc: 0.897 | Val loss: 1.1160  acc: 0.893\n",
            "P2 Epoch 04 | Train loss: 1.1139  acc: 0.905 | Val loss: 1.1147  acc: 0.889\n",
            "P2 Epoch 05 | Train loss: 1.1068  acc: 0.904 | Val loss: 1.1210  acc: 0.885\n",
            "[Phase 2] Test loss: 1.1828  |  Test accuracy: 0.872\n"
          ]
        }
      ],
      "source": [
        "PHASE2_EPOCHS = 5\n",
        "\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.features.parameters(),   'lr': 1e-6, 'weight_decay': 1e-4},\n",
        "    {'params': model.classifier.parameters(), 'lr': 1e-5, 'weight_decay': 1e-4},\n",
        "])\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=PHASE2_EPOCHS)\n",
        "\n",
        "for epoch in range(1, PHASE2_EPOCHS + 1):\n",
        "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
        "    val_loss,   val_acc   = run_epoch(val_loader,   train=False)\n",
        "    scheduler.step()\n",
        "    print(f'P2 Epoch {epoch:02d} | '\n",
        "          f'Train loss: {train_loss:.4f}  acc: {train_acc:.3f} | '\n",
        "          f'Val loss: {val_loss:.4f}  acc: {val_acc:.3f}')\n",
        "\n",
        "test_loss, test_acc = run_epoch(test_loader, train=False)\n",
        "print(f'[Phase 2] Test loss: {test_loss:.4f}  |  Test accuracy: {test_acc:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3b61a22",
      "metadata": {},
      "source": [
        "## 7. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "b872dae6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved.\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), 'pet_detective_mobilenetv2.pth')\n",
        "print('Model saved.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
